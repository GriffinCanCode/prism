# PLT-002: Lexical Analysis & Tokenization

**Document ID**: PLT-002  
**Status**: Draft  
**Type**: Core Compiler Component  
**Author**: Prism Language Team  
**Created**: 2025-01-17  
**Last Modified**: 2025-01-17  

## Document Metadata

| Field | Value |
|-------|-------|
| **Component Area** | Compiler Frontend |
| **Priority** | Core |
| **Dependencies** | PLD-001, PLD-002, PLD-003, PLD-004, PSG-001, PSG-002, PSG-003, PLT-001 |
| **Implementation Phase** | 1 |
| **Stability** | Experimental |

## Abstract

The Lexical Analysis & Tokenization system provides the foundational layer for Prism's AI-first compiler architecture, fully integrated with the Semantic Type System (PLD-001), Smart Module System (PLD-002), Effect System & Capabilities (PLD-003), and comprehensive style guide standards (PSG-001, PSG-002, PSG-003). This document specifies a multi-syntax lexer that supports C-like, Python-like, Rust-like, and canonical syntax styles while generating rich semantic metadata for AI comprehension. The lexer prioritizes conceptual cohesion measurement, semantic context extraction, and documentation validation as core language features, seamlessly integrating with the AST Design & Parser Architecture (PLT-001).

## Table of Contents

1. [Architecture Overview](#architecture-overview)
2. [Multi-Syntax Lexer Design](#multi-syntax-lexer-design)
3. [Semantic Token Generation](#semantic-token-generation)
4. [AI-First Tokenization](#ai-first-tokenization)
5. [Implementation Details](#implementation-details)
6. [Performance Considerations](#performance-considerations)
7. [Testing Strategy](#testing-strategy)
8. [Integration Points](#integration-points)
9. [Open Issues](#open-issues)
10. [References](#references)
11. [Appendices](#appendices)

## Architecture Overview

### High-Level Design

The Prism lexer forms the critical first stage of the AI-first compilation pipeline, fully integrated with all language subsystems:

```
Source Code (Multiple Syntax Styles)
     ↓
Syntax Detection & Style Analysis (PSG-001)
     ↓
Multi-Syntax Lexer (PLT-002) ← This Document
     ↓
Rich Semantic Token Stream
     ↓
Semantic Context Extraction (PLD-001/002/003)
     ↓
Documentation Validation (PSG-003)
     ↓
Multi-Syntax Parser (PLT-001)
     ↓
Rich Semantic AST
```

### Key Design Decisions

1. **Multi-Syntax Support**: Lex C-like, Python-like, Rust-like, and canonical syntax (PSG-001)
2. **Semantic-First Tokens**: Every token carries semantic context and AI metadata
3. **Responsibility Extraction**: Parse and validate @responsibility annotations (PSG-002/003)
4. **Effect System Integration**: Identify and categorize effect-related tokens (PLD-003)
5. **Documentation as Code**: Validate required annotations during lexing (PSG-003)
6. **AI-Readable Metadata**: Generate comprehensive semantic context for AI systems
7. **Conceptual Cohesion**: Extract cohesion metrics during tokenization (PLD-002)
8. **Universal Accessibility**: Support screen readers and assistive technologies (PSG-001)

## Multi-Syntax Lexer Design

### 1. Syntax Detection & Style Recognition

#### 1.1 Automatic Syntax Detection

The lexer automatically detects input syntax style based on structural patterns:

```rust
#[derive(Debug, Clone, PartialEq)]
pub enum SyntaxStyle {
    CLike,      // C/C++/Java/JavaScript: braces, semicolons, parentheses
    PythonLike, // Python/CoffeeScript: indentation, colons
    RustLike,   // Rust/Go: explicit keywords, snake_case
    Canonical,  // Prism canonical: semantic delimiters
    Mixed,      // Multiple styles in same file (warning)
}

#[derive(Debug, Clone)]
pub struct SyntaxDetector {
    pub detected_style: SyntaxStyle,
    pub confidence: f64,
    pub evidence: Vec<SyntaxEvidence>,
    pub mixed_style_warnings: Vec<MixedStyleWarning>,
}

#[derive(Debug, Clone)]
pub struct SyntaxEvidence {
    pub pattern: String,
    pub style: SyntaxStyle,
    pub weight: f64,
    pub location: Span,
}

impl SyntaxDetector {
    /// Detect syntax style from source code
    pub fn detect_syntax(source: &str) -> SyntaxDetector {
        let mut detector = SyntaxDetector {
            detected_style: SyntaxStyle::Canonical,
            confidence: 0.0,
            evidence: Vec::new(),
            mixed_style_warnings: Vec::new(),
        };
        
        // Analyze patterns
        detector.analyze_indentation_patterns(source);
        detector.analyze_brace_patterns(source);
        detector.analyze_keyword_patterns(source);
        detector.analyze_naming_patterns(source);
        detector.analyze_operator_patterns(source);
        
        // Calculate confidence and detect mixed styles
        detector.calculate_confidence();
        detector.detect_mixed_styles();
        
        detector
    }
    
    fn analyze_indentation_patterns(&mut self, source: &str) {
        // Look for Python-like indentation patterns
        let indentation_lines = source.lines()
            .enumerate()
            .filter(|(_, line)| line.trim_start().len() < line.len())
            .count();
            
        if indentation_lines > 0 {
            let ratio = indentation_lines as f64 / source.lines().count() as f64;
            if ratio > 0.3 {
                self.evidence.push(SyntaxEvidence {
                    pattern: "Significant indentation usage".to_string(),
                    style: SyntaxStyle::PythonLike,
                    weight: ratio * 0.8,
                    location: Span::entire_file(),
                });
            }
        }
    }
    
    fn analyze_brace_patterns(&mut self, source: &str) {
        // Look for C-like brace patterns
        let brace_count = source.chars().filter(|&c| c == '{' || c == '}').count();
        let line_count = source.lines().count();
        
        if brace_count > 0 {
            let ratio = brace_count as f64 / line_count as f64;
            if ratio > 0.1 {
                self.evidence.push(SyntaxEvidence {
                    pattern: "Frequent brace usage".to_string(),
                    style: SyntaxStyle::CLike,
                    weight: ratio * 0.7,
                    location: Span::entire_file(),
                });
            }
        }
    }
    
    fn analyze_keyword_patterns(&mut self, source: &str) {
        // Look for Rust-like keywords
        let rust_keywords = ["fn", "mod", "impl", "trait", "enum", "struct"];
        let rust_count = rust_keywords.iter()
            .map(|&keyword| source.matches(keyword).count())
            .sum::<usize>();
            
        if rust_count > 0 {
            self.evidence.push(SyntaxEvidence {
                pattern: "Rust-like keywords".to_string(),
                style: SyntaxStyle::RustLike,
                weight: rust_count as f64 * 0.1,
                location: Span::entire_file(),
            });
        }
        
        // Look for Prism canonical keywords
        let prism_keywords = ["function", "module", "section", "capability"];
        let prism_count = prism_keywords.iter()
            .map(|&keyword| source.matches(keyword).count())
            .sum::<usize>();
            
        if prism_count > 0 {
            self.evidence.push(SyntaxEvidence {
                pattern: "Prism canonical keywords".to_string(),
                style: SyntaxStyle::Canonical,
                weight: prism_count as f64 * 0.2,
                location: Span::entire_file(),
            });
        }
    }
}
```

#### 1.2 Style-Specific Lexing Rules

```rust
#[derive(Debug, Clone)]
pub struct StyleRules {
    pub indentation_semantic: bool,     // Python-like: indentation has meaning
    pub semicolon_required: bool,       // C-like: semicolons required
    pub brace_style: BraceStyle,        // How braces are handled
    pub operator_style: OperatorStyle,  // Symbolic vs English operators
    pub naming_convention: NamingStyle, // snake_case vs camelCase
}

#[derive(Debug, Clone)]
pub enum BraceStyle {
    Required,    // C-like: braces required for structure
    Optional,    // Python-like: braces optional, indentation used
    Semantic,    // Canonical: braces are semantic delimiters
}

#[derive(Debug, Clone)]
pub enum OperatorStyle {
    Symbolic,    // C-like: &&, ||, !
    English,     // Canonical: and, or, not
    Mixed,       // Both allowed
}

#[derive(Debug, Clone)]
pub enum NamingStyle {
    SnakeCase,   // Rust-like: snake_case
    CamelCase,   // C-like: camelCase
    PascalCase,  // Types: PascalCase
    Mixed,       // Context-dependent (PSG-002)
}

impl StyleRules {
    pub fn for_style(style: SyntaxStyle) -> Self {
        match style {
            SyntaxStyle::CLike => StyleRules {
                indentation_semantic: false,
                semicolon_required: true,
                brace_style: BraceStyle::Required,
                operator_style: OperatorStyle::Symbolic,
                naming_convention: NamingStyle::CamelCase,
            },
            SyntaxStyle::PythonLike => StyleRules {
                indentation_semantic: true,
                semicolon_required: false,
                brace_style: BraceStyle::Optional,
                operator_style: OperatorStyle::English,
                naming_convention: NamingStyle::SnakeCase,
            },
            SyntaxStyle::RustLike => StyleRules {
                indentation_semantic: false,
                semicolon_required: false,
                brace_style: BraceStyle::Required,
                operator_style: OperatorStyle::Symbolic,
                naming_convention: NamingStyle::SnakeCase,
            },
            SyntaxStyle::Canonical => StyleRules {
                indentation_semantic: false,
                semicolon_required: false,
                brace_style: BraceStyle::Semantic,
                operator_style: OperatorStyle::English,
                naming_convention: NamingStyle::Mixed,
            },
            SyntaxStyle::Mixed => StyleRules::for_style(SyntaxStyle::Canonical),
        }
    }
}
```

### 2. Token Types with Semantic Integration

#### 2.1 Core Token Structure

```rust
/// A token with comprehensive semantic metadata for AI comprehension
#[derive(Debug, Clone, PartialEq)]
pub struct Token {
    /// The token type and value
    pub kind: TokenKind,
    /// Source location with syntax style context
    pub span: Span,
    /// AI-readable semantic context
    pub semantic_context: Option<SemanticContext>,
    /// Syntax style this token was parsed from
    pub source_style: SyntaxStyle,
    /// Canonical representation
    pub canonical_form: Option<String>,
    /// Documentation validation status (PSG-003)
    pub doc_validation: Option<DocValidationStatus>,
    /// Responsibility annotation context (PSG-002)
    pub responsibility_context: Option<ResponsibilityContext>,
    /// Effect system context (PLD-003)
    pub effect_context: Option<EffectContext>,
    /// Cohesion metrics contribution (PLD-002)
    pub cohesion_impact: Option<CohesionImpact>,
}

impl Token {
    /// Create a new token with basic information
    pub fn new(kind: TokenKind, span: Span, source_style: SyntaxStyle) -> Self {
        Self {
            kind,
            span,
            semantic_context: None,
            source_style,
            canonical_form: None,
            doc_validation: None,
            responsibility_context: None,
            effect_context: None,
            cohesion_impact: None,
        }
    }
    
    /// Create a token with full semantic context
    pub fn with_full_context(
        kind: TokenKind,
        span: Span,
        source_style: SyntaxStyle,
        semantic_context: SemanticContext,
        canonical_form: Option<String>,
    ) -> Self {
        Self {
            kind,
            span,
            semantic_context: Some(semantic_context),
            source_style,
            canonical_form,
            doc_validation: None,
            responsibility_context: None,
            effect_context: None,
            cohesion_impact: None,
        }
    }
    
    /// Check if this token requires documentation validation
    pub fn requires_doc_validation(&self) -> bool {
        matches!(
            self.kind,
            TokenKind::Function | TokenKind::Module | TokenKind::Type | TokenKind::Public
        )
    }
    
    /// Check if this token contributes to conceptual cohesion
    pub fn affects_cohesion(&self) -> bool {
        matches!(
            self.kind,
            TokenKind::Function | TokenKind::Type | TokenKind::Identifier(_) | TokenKind::Module
        )
    }
    
    /// Get canonical representation of this token
    pub fn to_canonical(&self) -> String {
        self.canonical_form.clone().unwrap_or_else(|| {
            match &self.kind {
                TokenKind::Fn => "function".to_string(),
                TokenKind::AndAnd => "and".to_string(),
                TokenKind::OrOr => "or".to_string(),
                TokenKind::Bang => "not".to_string(),
                _ => self.kind.to_string(),
            }
        })
    }
}
```

#### 2.2 Extended Token Types

```rust
/// Comprehensive token types supporting all syntax styles and semantic features
#[derive(Debug, Clone, PartialEq, Logos)]
pub enum TokenKind {
    // === STRUCTURAL KEYWORDS ===
    
    // Module system (PLD-002 integration)
    #[token("module")]
    #[token("mod")]  // Rust-like alternative
    Module,
    
    #[token("section")]
    Section,
    
    #[token("capability")]
    #[token("cap")]  // Shortened form (PSG-002)
    Capability,
    
    // Function definitions (PSG-002 brevity + extended forms)
    #[token("function")]
    Function,
    #[token("fn")]
    Fn,
    
    // Type definitions (PLD-001 integration)
    #[token("type")]
    Type,
    #[token("interface")]
    Interface,
    #[token("trait")]  // Rust-like alternative
    Trait,
    
    // === VARIABLE DECLARATIONS ===
    
    #[token("let")]
    Let,
    #[token("const")]
    Const,
    #[token("var")]
    Var,
    
    // === CONTROL FLOW ===
    
    #[token("if")]
    If,
    #[token("else")]
    Else,
    #[token("elif")]  // Python-like
    #[token("else if")]  // Multi-token handling
    ElseIf,
    
    #[token("while")]
    While,
    #[token("for")]
    For,
    #[token("loop")]  // Rust-like
    Loop,
    
    #[token("match")]
    Match,
    #[token("case")]
    Case,
    #[token("when")]  // Alternative form
    When,
    
    #[token("return")]
    Return,
    #[token("break")]
    Break,
    #[token("continue")]
    Continue,
    #[token("yield")]
    Yield,
    
    // === LOGICAL OPERATORS (PSG-001 English preference) ===
    
    #[token("and")]
    And,
    #[token("&&")]  // C-like alternative
    AndAnd,
    
    #[token("or")]
    Or,
    #[token("||")]  // C-like alternative
    OrOr,
    
    #[token("not")]
    Not,
    #[token("!")]  // C-like alternative
    Bang,
    
    // === SEMANTIC TYPE KEYWORDS (PLD-001 integration) ===
    
    #[token("where")]
    Where,
    #[token("with")]
    With,
    #[token("requires")]
    Requires,
    #[token("ensures")]
    Ensures,
    #[token("invariant")]
    Invariant,
    
    // === EFFECT SYSTEM KEYWORDS (PLD-003 integration) ===
    
    #[token("effects")]
    Effects,
    #[token("capability")]
    CapabilityKeyword,
    #[token("secure")]
    Secure,
    #[token("unsafe")]
    Unsafe,
    
    // === ASYNC/CONCURRENCY ===
    
    #[token("async")]
    Async,
    #[token("await")]
    Await,
    #[token("sync")]
    Sync,
    
    // === ERROR HANDLING ===
    
    #[token("try")]
    Try,
    #[token("catch")]
    Catch,
    #[token("finally")]
    Finally,
    #[token("throw")]
    Throw,
    #[token("error")]
    Error,
    #[token("result")]
    Result,
    
    // === MODULE SYSTEM ===
    
    #[token("import")]
    Import,
    #[token("export")]
    Export,
    #[token("use")]  // Rust-like
    Use,
    #[token("from")]  // Python-like
    From,
    
    // === VISIBILITY (PLD-003 security integration) ===
    
    #[token("public")]
    Public,
    #[token("pub")]  // Rust-like
    Pub,
    #[token("private")]
    Private,
    #[token("internal")]
    Internal,
    #[token("protected")]
    Protected,
    
    // === SECTION TYPES (PLD-002 Smart Module integration) ===
    
    #[token("config")]
    Config,
    #[token("types")]
    Types,
    #[token("errors")]
    Errors,
    #[token("interface")]
    InterfaceSection,
    #[token("events")]
    Events,
    #[token("lifecycle")]
    Lifecycle,
    #[token("tests")]
    Tests,
    #[token("examples")]
    Examples,
    #[token("performance")]
    Performance,
    
    // === LITERALS ===
    
    #[token("true")]
    True,
    #[token("false")]
    False,
    #[token("null")]
    Null,
    #[token("nil")]  // Alternative
    Nil,
    #[token("undefined")]
    Undefined,
    
    // === MISC KEYWORDS ===
    
    #[token("in")]
    In,
    #[token("as")]
    As,
    #[token("is")]
    Is,
    #[token("typeof")]
    Typeof,
    #[token("sizeof")]
    Sizeof,
    
    // === COMPLEX LITERALS (Custom lexing required) ===
    
    /// Identifier with semantic context
    Identifier(IdentifierInfo),
    
    /// Integer literal with type hint
    IntegerLiteral(IntegerInfo),
    
    /// Float literal with precision info
    FloatLiteral(FloatInfo),
    
    /// String literal with encoding info
    StringLiteral(StringInfo),
    
    /// Boolean literal
    BooleanLiteral(bool),
    
    /// Regular expression literal
    RegexLiteral(RegexInfo),
    
    /// Money literal (semantic type)
    MoneyLiteral(MoneyInfo),
    
    /// Duration literal (semantic type)
    DurationLiteral(DurationInfo),
    
    // === OPERATORS ===
    
    // Arithmetic
    #[token("+")]
    Plus,
    #[token("-")]
    Minus,
    #[token("*")]
    Star,
    #[token("/")]
    Slash,
    #[token("%")]
    Percent,
    #[token("**")]
    Power,
    #[token("//")]  // Integer division
    IntegerDivision,
    
    // Comparison
    #[token("==")]
    Equal,
    #[token("!=")]
    NotEqual,
    #[token("<")]
    Less,
    #[token("<=")]
    LessEqual,
    #[token(">")]
    Greater,
    #[token(">=")]
    GreaterEqual,
    
    // Semantic comparison (PLD-001)
    #[token("===")]
    SemanticEqual,
    #[token("!==")]
    SemanticNotEqual,
    #[token("~=")]
    TypeCompatible,
    #[token("≈")]
    ConceptuallySimilar,
    
    // Assignment
    #[token("=")]
    Assign,
    #[token("+=")]
    PlusAssign,
    #[token("-=")]
    MinusAssign,
    #[token("*=")]
    StarAssign,
    #[token("/=")]
    SlashAssign,
    #[token("%=")]
    PercentAssign,
    
    // Bitwise
    #[token("&")]
    Ampersand,
    #[token("|")]
    Pipe,
    #[token("^")]
    Caret,
    #[token("<<")]
    LeftShift,
    #[token(">>")]
    RightShift,
    #[token("~")]
    Tilde,
    
    // === DELIMITERS ===
    
    #[token("(")]
    LeftParen,
    #[token(")")]
    RightParen,
    #[token("[")]
    LeftBracket,
    #[token("]")]
    RightBracket,
    #[token("{")]
    LeftBrace,
    #[token("}")]
    RightBrace,
    
    // === PUNCTUATION ===
    
    #[token(",")]
    Comma,
    #[token(";")]
    Semicolon,
    #[token(":")]
    Colon,
    #[token("::")]
    DoubleColon,
    #[token(".")]
    Dot,
    #[token("..")]
    DotDot,
    #[token("...")]
    DotDotDot,
    #[token("->")]
    Arrow,
    #[token("=>")]
    FatArrow,
    #[token("?")]
    Question,
    #[token("??")]
    DoubleQuestion,
    #[token("@")]
    At,
    #[token("#")]
    Hash,
    #[token("$")]
    Dollar,
    
    // === SPECIAL TOKENS ===
    
    /// Newline (significant in Python-like syntax)
    #[token("\n")]
    Newline,
    
    /// Indentation token (Python-like syntax)
    Indent(IndentInfo),
    
    /// Dedentation token (Python-like syntax)
    Dedent(DedentInfo),
    
    /// Whitespace (usually skipped, but preserved for formatting)
    #[regex(r"[ \t\r]+")]
    Whitespace(WhitespaceInfo),
    
    // === COMMENTS & DOCUMENTATION ===
    
    /// Line comment
    #[regex(r"//[^\n]*")]
    LineComment(CommentInfo),
    
    /// Block comment
    #[regex(r"/\*([^*]|\*[^/])*\*/")]
    BlockComment(CommentInfo),
    
    /// Documentation comment (PSG-003)
    #[regex(r"///[^\n]*")]
    DocComment(DocCommentInfo),
    
    /// Multi-line documentation comment
    #[regex(r"/\*\*([^*]|\*[^/])*\*/")]
    DocBlockComment(DocCommentInfo),
    
    /// AI annotation comment (preserved for semantic analysis)
    #[regex(r"@ai[^\n]*")]
    AiAnnotation(AiAnnotationInfo),
    
    /// Responsibility annotation (PSG-002/003)
    #[regex(r"@responsibility[^\n]*")]
    ResponsibilityAnnotation(ResponsibilityInfo),
    
    /// Effect annotation (PLD-003)
    #[regex(r"@effects[^\n]*")]
    EffectAnnotation(EffectInfo),
    
    /// Capability annotation (PLD-003)
    #[regex(r"@capability[^\n]*")]
    CapabilityAnnotation(CapabilityInfo),
    
    // === ERROR HANDLING ===
    
    /// Lexer error token
    LexError(LexErrorInfo),
    
    /// End of file
    Eof,
}
```

#### 2.3 Semantic Information Structures

```rust
/// Comprehensive semantic context for AI comprehension
#[derive(Debug, Clone, PartialEq)]
pub struct SemanticContext {
    /// Business purpose of this token
    pub purpose: Option<String>,
    /// Semantic domain (e.g., "authentication", "payment")
    pub domain: Option<String>,
    /// Related concepts for AI understanding
    pub related_concepts: Vec<String>,
    /// Security implications
    pub security_implications: Vec<String>,
    /// Business rules that apply
    pub business_rules: Vec<String>,
    /// Compliance requirements
    pub compliance_requirements: Vec<String>,
    /// Performance characteristics
    pub performance_notes: Vec<String>,
    /// AI-specific hints
    pub ai_hints: Vec<String>,
}

/// Identifier information with semantic context
#[derive(Debug, Clone, PartialEq)]
pub struct IdentifierInfo {
    /// The identifier name
    pub name: String,
    /// Naming convention used
    pub naming_style: NamingStyle,
    /// Semantic category
    pub category: IdentifierCategory,
    /// Brevity vs clarity context (PSG-002)
    pub brevity_context: BrevityContext,
    /// Extended form if available
    pub extended_form: Option<String>,
}

#[derive(Debug, Clone, PartialEq)]
pub enum IdentifierCategory {
    ModuleName,
    FunctionName,
    TypeName,
    VariableName,
    ConstantName,
    ParameterName,
    FieldName,
    AnnotationName,
    Unknown,
}

#[derive(Debug, Clone, PartialEq)]
pub struct BrevityContext {
    /// Scope level (local favors brevity)
    pub scope_level: ScopeLevel,
    /// Audience (public API favors clarity)
    pub audience: AudienceType,
    /// Context clarity (how much context is available)
    pub context_clarity: f64,
    /// Recommended form
    pub recommended_form: RecommendedForm,
}

#[derive(Debug, Clone, PartialEq)]
pub enum ScopeLevel {
    Local,      // Function/block scope
    Module,     // Module scope
    Public,     // Public API
    Domain,     // Business domain
}

#[derive(Debug, Clone, PartialEq)]
pub enum AudienceType {
    Internal,   // Team/organization
    Public,     // External API
    Business,   // Business stakeholders
    AI,         // AI systems
}

#[derive(Debug, Clone, PartialEq)]
pub enum RecommendedForm {
    Brief,      // Short form recommended
    Extended,   // Extended form recommended
    Either,     // Both forms acceptable
}

/// Documentation validation status (PSG-003)
#[derive(Debug, Clone, PartialEq)]
pub struct DocValidationStatus {
    /// Required annotations present
    pub required_annotations: Vec<RequiredAnnotationStatus>,
    /// Documentation completeness
    pub completeness_score: f64,
    /// Validation errors
    pub validation_errors: Vec<DocValidationError>,
    /// AI comprehension score
    pub ai_comprehension_score: f64,
}

#[derive(Debug, Clone, PartialEq)]
pub struct RequiredAnnotationStatus {
    pub annotation_type: RequiredAnnotationType,
    pub present: bool,
    pub valid: bool,
    pub error_message: Option<String>,
}

#[derive(Debug, Clone, PartialEq)]
pub enum RequiredAnnotationType {
    Responsibility,     // @responsibility
    Description,        // @description
    Author,            // @author
    Param,             // @param
    Returns,           // @returns
    Throws,            // @throws
    Effects,           // @effects
    Example,           // @example
    AiContext,         // @aiContext
}

/// Responsibility annotation context (PSG-002)
#[derive(Debug, Clone, PartialEq)]
pub struct ResponsibilityContext {
    /// The responsibility statement
    pub statement: String,
    /// Length validation (max 80 chars)
    pub length_valid: bool,
    /// Clarity score
    pub clarity_score: f64,
    /// Single responsibility principle adherence
    pub srp_adherence: f64,
    /// Suggested improvements
    pub suggestions: Vec<String>,
}

/// Effect system context (PLD-003)
#[derive(Debug, Clone, PartialEq)]
pub struct EffectContext {
    /// Effects this token contributes to
    pub effects: Vec<EffectType>,
    /// Capabilities required
    pub capabilities_required: Vec<CapabilityType>,
    /// Security level
    pub security_level: SecurityLevel,
    /// Audit requirements
    pub audit_required: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub enum EffectType {
    IO,
    Network,
    System,
    Cryptography,
    Database,
    Memory,
    Computation,
    Custom(String),
}

#[derive(Debug, Clone, PartialEq)]
pub enum CapabilityType {
    FileSystem,
    Network,
    System,
    Cryptography,
    Database,
    Memory,
    UnsafeOperations,
    Custom(String),
}

#[derive(Debug, Clone, PartialEq)]
pub enum SecurityLevel {
    Public,     // No security implications
    Internal,   // Internal use only
    Restricted, // Restricted access
    Classified, // Classified information
}

/// Conceptual cohesion impact (PLD-002)
#[derive(Debug, Clone, PartialEq)]
pub struct CohesionImpact {
    /// How this token affects type cohesion
    pub type_cohesion_impact: f64,
    /// How this token affects data flow cohesion
    pub data_flow_impact: f64,
    /// How this token affects semantic cohesion
    pub semantic_impact: f64,
    /// Related concepts
    pub related_concepts: Vec<String>,
    /// Conceptual distance from other tokens
    pub conceptual_distance: f64,
}
```

## Semantic Token Generation

### 3. AI-First Tokenization Process

#### 3.1 Semantic Context Extraction

```rust
/// Semantic lexer that enriches tokens with AI-comprehensible metadata
pub struct SemanticLexer<'source> {
    /// Base lexer for tokenization
    lexer: Lexer<'source>,
    /// Semantic context stack
    context_stack: Vec<SemanticContext>,
    /// Current module context
    current_module: Option<ModuleContext>,
    /// Documentation validation state
    doc_validator: DocumentationValidator,
    /// Responsibility tracker
    responsibility_tracker: ResponsibilityTracker,
    /// Effect system analyzer
    effect_analyzer: EffectAnalyzer,
    /// Cohesion metrics calculator
    cohesion_calculator: CohesionCalculator,
}

impl<'source> SemanticLexer<'source> {
    /// Create a new semantic lexer
    pub fn new(
        source: &'source str,
        source_id: SourceId,
        symbol_table: &'source mut SymbolTable,
        config: LexerConfig,
    ) -> Self {
        Self {
            lexer: Lexer::new(source, source_id, symbol_table, config),
            context_stack: Vec::new(),
            current_module: None,
            doc_validator: DocumentationValidator::new(),
            responsibility_tracker: ResponsibilityTracker::new(),
            effect_analyzer: EffectAnalyzer::new(),
            cohesion_calculator: CohesionCalculator::new(),
        }
    }
    
    /// Tokenize with full semantic analysis
    pub fn tokenize_with_semantics(mut self) -> LexerResult {
        let mut tokens = Vec::new();
        let mut diagnostics = DiagnosticBag::new();
        
        // First pass: Basic tokenization
        let (basic_tokens, basic_diagnostics) = self.lexer.tokenize();
        diagnostics.extend(basic_diagnostics);
        
        // Second pass: Semantic enrichment
        for token in basic_tokens {
            let enriched_token = self.enrich_token(token)?;
            tokens.push(enriched_token);
        }
        
        // Third pass: Validation
        let validation_diagnostics = self.validate_tokens(&tokens);
        diagnostics.extend(validation_diagnostics);
        
        // Fourth pass: Cohesion analysis
        let cohesion_metrics = self.calculate_cohesion_metrics(&tokens);
        
        Ok(LexerResult {
            tokens,
            diagnostics,
            syntax_style: self.lexer.detected_style,
            cohesion_metrics,
            semantic_summary: self.generate_semantic_summary(),
        })
    }
    
    /// Enrich a token with semantic context
    fn enrich_token(&mut self, mut token: Token) -> Result<Token, LexerError> {
        // Add semantic context based on token type
        token.semantic_context = Some(self.infer_semantic_context(&token)?);
        
        // Add canonical form
        token.canonical_form = Some(self.generate_canonical_form(&token));
        
        // Add documentation validation
        if token.requires_doc_validation() {
            token.doc_validation = Some(self.doc_validator.validate_token(&token)?);
        }
        
        // Add responsibility context
        if self.is_responsibility_token(&token) {
            token.responsibility_context = Some(self.responsibility_tracker.analyze_token(&token)?);
        }
        
        // Add effect context
        if self.is_effect_token(&token) {
            token.effect_context = Some(self.effect_analyzer.analyze_token(&token)?);
        }
        
        // Add cohesion impact
        if token.affects_cohesion() {
            token.cohesion_impact = Some(self.cohesion_calculator.calculate_impact(&token)?);
        }
        
        Ok(token)
    }
    
    /// Infer semantic context for a token
    fn infer_semantic_context(&self, token: &Token) -> Result<SemanticContext, LexerError> {
        let mut context = SemanticContext::new();
        
        match &token.kind {
            TokenKind::Module => {
                context.purpose = Some("Define module boundary and capabilities".to_string());
                context.domain = Some("Module System".to_string());
                context.related_concepts = vec![
                    "Conceptual Cohesion".to_string(),
                    "Capability Isolation".to_string(),
                    "Smart Modules".to_string(),
                ];
                context.ai_hints = vec![
                    "Modules represent single business capabilities".to_string(),
                    "Each module should have high conceptual cohesion".to_string(),
                ];
            }
            TokenKind::Function => {
                context.purpose = Some("Define function with semantic contracts".to_string());
                context.domain = Some("Function Definition".to_string());
                context.related_concepts = vec![
                    "Semantic Types".to_string(),
                    "Effect System".to_string(),
                    "Contract Programming".to_string(),
                ];
                context.ai_hints = vec![
                    "Functions should have single responsibility".to_string(),
                    "All public functions require documentation".to_string(),
                ];
            }
            TokenKind::Type => {
                context.purpose = Some("Define semantic type with constraints".to_string());
                context.domain = Some("Type System".to_string());
                context.related_concepts = vec![
                    "Semantic Types".to_string(),
                    "Business Rules".to_string(),
                    "Domain Modeling".to_string(),
                ];
                context.ai_hints = vec![
                    "Types should express business meaning".to_string(),
                    "Include validation constraints".to_string(),
                ];
            }
            TokenKind::Effects => {
                context.purpose = Some("Declare computational effects".to_string());
                context.domain = Some("Effect System".to_string());
                context.related_concepts = vec![
                    "Capability-Based Security".to_string(),
                    "Side Effects".to_string(),
                    "Resource Access".to_string(),
                ];
                context.security_implications = vec![
                    "Effects must be explicitly declared".to_string(),
                    "Capability requirements enforced".to_string(),
                ];
            }
            TokenKind::Identifier(info) => {
                context = self.infer_identifier_context(info)?;
            }
            _ => {
                // Default context for other tokens
                context.purpose = Some(format!("Language construct: {}", token.kind));
            }
        }
        
        Ok(context)
    }
    
    /// Infer semantic context for identifiers
    fn infer_identifier_context(&self, info: &IdentifierInfo) -> Result<SemanticContext, LexerError> {
        let mut context = SemanticContext::new();
        
        match info.category {
            IdentifierCategory::ModuleName => {
                context.purpose = Some("Module name representing business capability".to_string());
                context.domain = Some("Module System".to_string());
                context.ai_hints = vec![
                    "Should represent a single business capability".to_string(),
                    "Use PascalCase naming convention".to_string(),
                ];
            }
            IdentifierCategory::FunctionName => {
                context.purpose = Some("Function name describing operation".to_string());
                context.domain = Some("Function Definition".to_string());
                
                // Apply PSG-002 linguistic modifier analysis
                if let Some(modifiers) = self.analyze_linguistic_modifiers(&info.name) {
                    context.ai_hints.extend(modifiers);
                }
                
                // Check brevity context
                match info.brevity_context.recommended_form {
                    RecommendedForm::Brief => {
                        context.ai_hints.push("Brief form recommended for this context".to_string());
                    }
                    RecommendedForm::Extended => {
                        context.ai_hints.push("Extended form recommended for clarity".to_string());
                    }
                    RecommendedForm::Either => {
                        context.ai_hints.push("Both brief and extended forms acceptable".to_string());
                    }
                }
            }
            IdentifierCategory::TypeName => {
                context.purpose = Some("Type name representing domain concept".to_string());
                context.domain = Some("Type System".to_string());
                context.ai_hints = vec![
                    "Should represent business domain concept".to_string(),
                    "Include semantic constraints where applicable".to_string(),
                ];
            }
            _ => {
                context.purpose = Some(format!("Identifier: {}", info.name));
            }
        }
        
        Ok(context)
    }
    
    /// Analyze linguistic modifiers in function names (PSG-002)
    fn analyze_linguistic_modifiers(&self, name: &str) -> Option<Vec<String>> {
        let mut hints = Vec::new();
        
        // Check for intensity modifiers
        if name.contains("strict") {
            hints.push("Strict modifier indicates rigorous validation".to_string());
        } else if name.contains("soft") {
            hints.push("Soft modifier indicates lenient validation".to_string());
        } else if name.contains("deep") {
            hints.push("Deep modifier indicates thorough/recursive operation".to_string());
        } else if name.contains("shallow") {
            hints.push("Shallow modifier indicates surface-level operation".to_string());
        }
        
        // Check for directional modifiers
        if name.contains("from") {
            hints.push("From modifier indicates source/origin operation".to_string());
        } else if name.contains("to") {
            hints.push("To modifier indicates destination/target operation".to_string());
        } else if name.contains("by") {
            hints.push("By modifier indicates method/agent operation".to_string());
        } else if name.contains("with") {
            hints.push("With modifier indicates accompaniment/tool operation".to_string());
        }
        
        // Check for quantitative modifiers
        if name.contains("all") {
            hints.push("All modifier indicates complete set operation".to_string());
        } else if name.contains("one") {
            hints.push("One modifier indicates single item operation".to_string());
        } else if name.contains("many") {
            hints.push("Many modifier indicates multiple items operation".to_string());
        }
        
        // Check for temporal modifiers
        if name.contains("now") {
            hints.push("Now modifier indicates immediate operation".to_string());
        } else if name.contains("later") {
            hints.push("Later modifier indicates deferred operation".to_string());
        } else if name.contains("before") {
            hints.push("Before modifier indicates prerequisite operation".to_string());
        } else if name.contains("after") {
            hints.push("After modifier indicates subsequent operation".to_string());
        }
        
        if hints.is_empty() {
            None
        } else {
            Some(hints)
        }
    }
}
```

#### 3.2 Documentation Validation Integration

```rust
/// Documentation validator for PSG-003 compliance
pub struct DocumentationValidator {
    /// Required annotations by context
    required_annotations: HashMap<TokenKind, Vec<RequiredAnnotationType>>,
    /// Validation rules
    validation_rules: Vec<ValidationRule>,
    /// Current validation state
    current_state: ValidationState,
}

impl DocumentationValidator {
    pub fn new() -> Self {
        let mut validator = Self {
            required_annotations: HashMap::new(),
            validation_rules: Vec::new(),
            current_state: ValidationState::new(),
        };
        
        validator.initialize_requirements();
        validator
    }
    
    fn initialize_requirements(&mut self) {
        // Module-level requirements (PSG-003)
        self.required_annotations.insert(
            TokenKind::Module,
            vec![
                RequiredAnnotationType::Responsibility,
                RequiredAnnotationType::Description,
                RequiredAnnotationType::Author,
            ],
        );
        
        // Function-level requirements (PSG-003)
        self.required_annotations.insert(
            TokenKind::Function,
            vec![
                RequiredAnnotationType::Responsibility,
                RequiredAnnotationType::Param,
                RequiredAnnotationType::Returns,
                RequiredAnnotationType::Example,
            ],
        );
        
        // Type-level requirements (PSG-003)
        self.required_annotations.insert(
            TokenKind::Type,
            vec![
                RequiredAnnotationType::Responsibility,
                RequiredAnnotationType::Example,
            ],
        );
    }
    
    pub fn validate_token(&mut self, token: &Token) -> Result<DocValidationStatus, LexerError> {
        let mut status = DocValidationStatus {
            required_annotations: Vec::new(),
            completeness_score: 0.0,
            validation_errors: Vec::new(),
            ai_comprehension_score: 0.0,
        };
        
        // Check required annotations
        if let Some(required) = self.required_annotations.get(&token.kind) {
            for annotation_type in required {
                let annotation_status = self.check_annotation_presence(token, annotation_type)?;
                status.required_annotations.push(annotation_status);
            }
        }
        
        // Calculate completeness score
        status.completeness_score = self.calculate_completeness_score(&status.required_annotations);
        
        // Calculate AI comprehension score
        status.ai_comprehension_score = self.calculate_ai_comprehension_score(token)?;
        
        Ok(status)
    }
    
    fn check_annotation_presence(
        &self,
        token: &Token,
        annotation_type: &RequiredAnnotationType,
    ) -> Result<RequiredAnnotationStatus, LexerError> {
        // This would check the preceding tokens for the required annotation
        // Implementation depends on the specific annotation type
        Ok(RequiredAnnotationStatus {
            annotation_type: annotation_type.clone(),
            present: false, // Placeholder
            valid: false,   // Placeholder
            error_message: None,
        })
    }
    
    fn calculate_completeness_score(&self, annotations: &[RequiredAnnotationStatus]) -> f64 {
        if annotations.is_empty() {
            return 100.0;
        }
        
        let valid_count = annotations.iter().filter(|a| a.present && a.valid).count();
        (valid_count as f64 / annotations.len() as f64) * 100.0
    }
    
    fn calculate_ai_comprehension_score(&self, token: &Token) -> Result<f64, LexerError> {
        let mut score = 0.0;
        
        // Base score for token type
        score += match token.kind {
            TokenKind::Module => 20.0,
            TokenKind::Function => 25.0,
            TokenKind::Type => 20.0,
            _ => 10.0,
        };
        
        // Bonus for semantic context
        if token.semantic_context.is_some() {
            score += 25.0;
        }
        
        // Bonus for documentation
        if token.doc_validation.is_some() {
            score += 25.0;
        }
        
        // Bonus for responsibility context
        if token.responsibility_context.is_some() {
            score += 15.0;
        }
        
        // Bonus for effect context
        if token.effect_context.is_some() {
            score += 15.0;
        }
        
        Ok(score.min(100.0))
    }
}
```

## AI-First Tokenization

### 4. Performance Considerations

#### 4.1 Incremental Lexing

```rust
/// Incremental lexer for real-time feedback
pub struct IncrementalLexer {
    /// Current token stream
    tokens: Vec<Token>,
    /// Dirty regions that need re-lexing
    dirty_regions: Vec<DirtyRegion>,
    /// Semantic context cache
    context_cache: HashMap<Span, SemanticContext>,
    /// Performance metrics
    metrics: LexingMetrics,
}

#[derive(Debug, Clone)]
pub struct DirtyRegion {
    pub start: Position,
    pub end: Position,
    pub reason: DirtyReason,
}

#[derive(Debug, Clone)]
pub enum DirtyReason {
    TextChange,
    AnnotationChange,
    ContextChange,
    ValidationChange,
}

impl IncrementalLexer {
    /// Update tokens for a text change
    pub fn update_for_change(
        &mut self,
        change: TextChange,
    ) -> Result<Vec<Token>, LexerError> {
        // Mark affected regions as dirty
        self.mark_dirty_region(change.span, DirtyReason::TextChange);
        
        // Re-lex dirty regions
        let updated_tokens = self.relex_dirty_regions()?;
        
        // Update semantic context cache
        self.update_context_cache(&updated_tokens)?;
        
        Ok(updated_tokens)
    }
    
    /// Re-lex only the dirty regions
    fn relex_dirty_regions(&mut self) -> Result<Vec<Token>, LexerError> {
        let mut updated_tokens = Vec::new();
        
        for dirty_region in &self.dirty_regions {
            let region_tokens = self.lex_region(dirty_region)?;
            updated_tokens.extend(region_tokens);
        }
        
        // Clear dirty regions
        self.dirty_regions.clear();
        
        Ok(updated_tokens)
    }
}
```

#### 4.2 Caching Strategy

```rust
/// Semantic context cache for performance
pub struct SemanticContextCache {
    /// Cache by token span
    span_cache: HashMap<Span, SemanticContext>,
    /// Cache by identifier name
    identifier_cache: HashMap<String, IdentifierInfo>,
    /// Cache by module context
    module_cache: HashMap<String, ModuleContext>,
    /// Cache statistics
    stats: CacheStats,
}

impl SemanticContextCache {
    /// Get cached semantic context
    pub fn get_context(&self, span: &Span) -> Option<&SemanticContext> {
        self.span_cache.get(span)
    }
    
    /// Cache semantic context
    pub fn cache_context(&mut self, span: Span, context: SemanticContext) {
        self.span_cache.insert(span, context);
        self.stats.cache_hits += 1;
    }
    
    /// Invalidate cache for region
    pub fn invalidate_region(&mut self, region: &DirtyRegion) {
        self.span_cache.retain(|span, _| !region.contains(span));
        self.stats.cache_invalidations += 1;
    }
}
```

## Implementation Details

### 5. Lexer Configuration

```rust
/// Configuration for lexer behavior
#[derive(Debug, Clone)]
pub struct LexerConfig {
    /// Syntax style preferences
    pub syntax_style: Option<SyntaxStyle>,
    /// Enable aggressive error recovery
    pub aggressive_recovery: bool,
    /// Extract AI context during lexing
    pub extract_ai_context: bool,
    /// Maximum number of errors before stopping
    pub max_errors: usize,
    /// Enable semantic metadata extraction
    pub semantic_metadata: bool,
    /// Enable documentation validation
    pub validate_documentation: bool,
    /// Enable responsibility tracking
    pub track_responsibilities: bool,
    /// Enable effect system analysis
    pub analyze_effects: bool,
    /// Enable cohesion metrics
    pub calculate_cohesion: bool,
    /// Performance optimization level
    pub optimization_level: OptimizationLevel,
}

#[derive(Debug, Clone)]
pub enum OptimizationLevel {
    Debug,      // Full analysis, slow
    Development, // Balanced analysis
    Production, // Minimal analysis, fast
}

impl Default for LexerConfig {
    fn default() -> Self {
        Self {
            syntax_style: None,
            aggressive_recovery: true,
            extract_ai_context: true,
            max_errors: 100,
            semantic_metadata: true,
            validate_documentation: true,
            track_responsibilities: true,
            analyze_effects: true,
            calculate_cohesion: true,
            optimization_level: OptimizationLevel::Development,
        }
    }
}
```

### 6. Error Recovery

```rust
/// Error recovery strategies for robust lexing
pub struct ErrorRecovery {
    /// Recovery strategies by error type
    strategies: HashMap<LexErrorKind, RecoveryStrategy>,
    /// Maximum recovery attempts
    max_attempts: usize,
    /// Current recovery state
    state: RecoveryState,
}

#[derive(Debug, Clone)]
pub enum RecoveryStrategy {
    /// Skip invalid token and continue
    Skip,
    /// Insert missing token
    Insert(TokenKind),
    /// Replace invalid token
    Replace(TokenKind),
    /// Synchronize to next valid token
    Synchronize,
    /// Custom recovery logic
    Custom(Box<dyn Fn(&Token) -> RecoveryAction>),
}

#[derive(Debug, Clone)]
pub enum RecoveryAction {
    Continue,
    Retry,
    Abort,
}

impl ErrorRecovery {
    /// Attempt to recover from lexer error
    pub fn recover_from_error(
        &mut self,
        error: &LexerError,
        context: &LexerContext,
    ) -> Result<RecoveryAction, LexerError> {
        if let Some(strategy) = self.strategies.get(&error.kind) {
            match strategy {
                RecoveryStrategy::Skip => {
                    // Skip the problematic token and continue
                    Ok(RecoveryAction::Continue)
                }
                RecoveryStrategy::Insert(token_kind) => {
                    // Insert the missing token
                    self.insert_token(token_kind.clone(), context)?;
                    Ok(RecoveryAction::Continue)
                }
                RecoveryStrategy::Replace(token_kind) => {
                    // Replace the invalid token
                    self.replace_token(token_kind.clone(), context)?;
                    Ok(RecoveryAction::Continue)
                }
                RecoveryStrategy::Synchronize => {
                    // Synchronize to next valid token
                    self.synchronize_to_next_valid(context)?;
                    Ok(RecoveryAction::Continue)
                }
                RecoveryStrategy::Custom(recovery_fn) => {
                    // Apply custom recovery logic
                    let dummy_token = Token::new(
                        TokenKind::LexError(LexErrorInfo::new(error.clone())),
                        error.span,
                        SyntaxStyle::Canonical,
                    );
                    Ok(recovery_fn(&dummy_token))
                }
            }
        } else {
            // No recovery strategy available
            Err(error.clone())
        }
    }
}
```

## Testing Strategy

### 7. Comprehensive Test Suite

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use prism_common::symbol::SymbolTable;
    use insta::assert_debug_snapshot;
    
    #[test]
    fn test_multi_syntax_detection() {
        let test_cases = vec![
            // C-like syntax
            ("module Test { function foo() { return 42; } }", SyntaxStyle::CLike),
            // Python-like syntax  
            ("module Test:\n    function foo():\n        return 42", SyntaxStyle::PythonLike),
            // Rust-like syntax
            ("mod test { fn foo() -> i32 { 42 } }", SyntaxStyle::RustLike),
            // Canonical syntax
            ("module Test { function foo() -> i32 { return 42 } }", SyntaxStyle::Canonical),
        ];
        
        for (source, expected_style) in test_cases {
            let detector = SyntaxDetector::detect_syntax(source);
            assert_eq!(detector.detected_style, expected_style);
            assert!(detector.confidence > 0.5);
        }
    }
    
    #[test]
    fn test_semantic_context_extraction() {
        let source = r#"
        @responsibility "Handles user authentication"
        @description "Secure authentication with MFA support"
        module UserAuth {
            function authenticate(email: Email, password: Password) -> Result<Session, AuthError>
                effects [Database.Query, Audit.Log]
        }
        "#;
        
        let source_id = SourceId::new(1);
        let mut symbol_table = SymbolTable::new();
        let config = LexerConfig::default();
        
        let lexer = SemanticLexer::new(source, source_id, &mut symbol_table, config);
        let result = lexer.tokenize_with_semantics().unwrap();
        
        // Verify semantic context is extracted
        let module_token = result.tokens.iter()
            .find(|t| matches!(t.kind, TokenKind::Module))
            .unwrap();
            
        assert!(module_token.semantic_context.is_some());
        assert!(module_token.responsibility_context.is_some());
        
        let function_token = result.tokens.iter()
            .find(|t| matches!(t.kind, TokenKind::Function))
            .unwrap();
            
        assert!(function_token.semantic_context.is_some());
        assert!(function_token.effect_context.is_some());
    }
    
    #[test]
    fn test_documentation_validation() {
        let source = r#"
        // Missing @responsibility annotation - should fail validation
        module UserAuth {
            // Missing documentation - should fail validation
            function authenticate() -> Result<Session, AuthError>
        }
        "#;
        
        let source_id = SourceId::new(1);
        let mut symbol_table = SymbolTable::new();
        let config = LexerConfig::default();
        
        let lexer = SemanticLexer::new(source, source_id, &mut symbol_table, config);
        let result = lexer.tokenize_with_semantics().unwrap();
        
        // Should have validation errors
        assert!(result.diagnostics.has_errors());
        
        // Check specific validation failures
        let validation_errors: Vec<_> = result.diagnostics.errors()
            .filter(|e| e.message.contains("Missing @responsibility"))
            .collect();
        assert!(!validation_errors.is_empty());
    }
    
    #[test]
    fn test_linguistic_modifiers() {
        let test_cases = vec![
            ("validateStrict", vec!["Strict modifier indicates rigorous validation"]),
            ("getAllDeep", vec!["All modifier indicates complete set operation", "Deep modifier indicates thorough/recursive operation"]),
            ("authByNow", vec!["By modifier indicates method/agent operation", "Now modifier indicates immediate operation"]),
        ];
        
        for (function_name, expected_hints) in test_cases {
            let source = format!("function {}() {{}}", function_name);
            let source_id = SourceId::new(1);
            let mut symbol_table = SymbolTable::new();
            let config = LexerConfig::default();
            
            let lexer = SemanticLexer::new(&source, source_id, &mut symbol_table, config);
            let result = lexer.tokenize_with_semantics().unwrap();
            
            let identifier_token = result.tokens.iter()
                .find(|t| matches!(t.kind, TokenKind::Identifier(_)))
                .unwrap();
                
            if let Some(context) = &identifier_token.semantic_context {
                for expected_hint in expected_hints {
                    assert!(context.ai_hints.contains(&expected_hint.to_string()));
                }
            }
        }
    }
    
    #[test]
    fn test_cohesion_metrics() {
        let source = r#"
        @responsibility "Handles user management operations"
        module UserManagement {
            section types {
                type User = { id: UserId, email: Email }
                type UserId = UUID tagged "User"
                type Email = String where { pattern: EMAIL_REGEX }
            }
            
            section interface {
                function createUser(email: Email) -> Result<User, UserError>
                function getUser(id: UserId) -> Result<User, UserError>
                function updateUser(user: User) -> Result<User, UserError>
            }
        }
        "#;
        
        let source_id = SourceId::new(1);
        let mut symbol_table = SymbolTable::new();
        let config = LexerConfig::default();
        
        let lexer = SemanticLexer::new(source, source_id, &mut symbol_table, config);
        let result = lexer.tokenize_with_semantics().unwrap();
        
        // Should have cohesion metrics
        assert!(result.cohesion_metrics.is_some());
        
        let metrics = result.cohesion_metrics.unwrap();
        assert!(metrics.overall_score > 0.0);
        assert!(metrics.type_cohesion > 0.0);
        assert!(metrics.semantic_cohesion > 0.0);
    }
    
    #[test]
    fn test_error_recovery() {
        let source = r#"
        module Test {
            function invalid_syntax(( -> Result<(), Error> {
                // Invalid syntax should be recovered
                return Ok(())
            }
        }
        "#;
        
        let source_id = SourceId::new(1);
        let mut symbol_table = SymbolTable::new();
        let config = LexerConfig {
            aggressive_recovery: true,
            ..Default::default()
        };
        
        let lexer = SemanticLexer::new(source, source_id, &mut symbol_table, config);
        let result = lexer.tokenize_with_semantics().unwrap();
        
        // Should have errors but continue lexing
        assert!(result.diagnostics.has_errors());
        assert!(!result.tokens.is_empty());
        
        // Should have EOF token
        assert!(matches!(result.tokens.last().unwrap().kind, TokenKind::Eof));
    }
    
    #[test]
    fn test_canonical_conversion() {
        let source = "fn test() -> i32 { 42 }";  // Rust-like syntax
        let source_id = SourceId::new(1);
        let mut symbol_table = SymbolTable::new();
        let config = LexerConfig::default();
        
        let lexer = SemanticLexer::new(source, source_id, &mut symbol_table, config);
        let result = lexer.tokenize_with_semantics().unwrap();
        
        // Find the fn token
        let fn_token = result.tokens.iter()
            .find(|t| matches!(t.kind, TokenKind::Fn))
            .unwrap();
            
        // Should have canonical form
        assert_eq!(fn_token.to_canonical(), "function");
    }
}
```

## Integration Points

### 8. Compiler Integration

#### 8.1 Query System Integration (PLD-004)

```rust
/// Integration with PLD-004 query-based compilation
impl QueryProvider for SemanticLexer<'_> {
    type Input = SourceCode;
    type Output = LexerResult;
    
    fn execute_query(&mut self, input: Self::Input) -> Result<Self::Output, QueryError> {
        // Tokenize with full semantic analysis
        let result = self.tokenize_with_semantics()
            .map_err(|e| QueryError::LexerError(e))?;
        
        Ok(result)
    }
    
    fn cache_key(&self, input: &Self::Input) -> CacheKey {
        // Generate cache key based on source content and configuration
        CacheKey::from_content_and_config(input, &self.config)
    }
    
    fn dependencies(&self, input: &Self::Input) -> Vec<QueryId> {
        // Lexer depends on source file and configuration
        vec![
            QueryId::SourceFile(input.source_id),
            QueryId::LexerConfig,
        ]
    }
    
    fn invalidate_on(&self, input: &Self::Input) -> Vec<InvalidationTrigger> {
        vec![
            InvalidationTrigger::SourceChange(input.source_id),
            InvalidationTrigger::ConfigChange,
        ]
    }
}
```

#### 8.2 Parser Integration (PLT-001)

```rust
/// Integration with PLT-001 parser architecture
pub struct LexerParserBridge {
    /// Lexer result
    lexer_result: LexerResult,
    /// Token stream for parser
    token_stream: TokenStream,
    /// Semantic context for parser
    semantic_context: SemanticContext,
}

impl LexerParserBridge {
    /// Create bridge from lexer result
    pub fn from_lexer_result(result: LexerResult) -> Self {
        let token_stream = TokenStream::from_tokens(result.tokens.clone());
        let semantic_context = SemanticContext::from_tokens(&result.tokens);
        
        Self {
            lexer_result: result,
            token_stream,
            semantic_context,
        }
    }
    
    /// Get token stream for parser
    pub fn token_stream(&self) -> &TokenStream {
        &self.token_stream
    }
    
    /// Get semantic context for parser
    pub fn semantic_context(&self) -> &SemanticContext {
        &self.semantic_context
    }
    
    /// Get cohesion metrics for parser
    pub fn cohesion_metrics(&self) -> Option<&CohesionMetrics> {
        self.lexer_result.cohesion_metrics.as_ref()
    }
}
```

## Performance Considerations

### 9. Optimization Strategies

#### 9.1 Lazy Evaluation

```rust
/// Lazy semantic analysis for performance
pub struct LazySemanticAnalysis {
    /// Tokens with lazy semantic context
    tokens: Vec<LazyToken>,
    /// Analysis cache
    cache: SemanticContextCache,
    /// Analysis queue
    analysis_queue: VecDeque<TokenId>,
}

#[derive(Debug, Clone)]
pub struct LazyToken {
    /// Basic token information
    pub basic: Token,
    /// Lazy semantic context
    pub semantic_context: LazySemanticContext,
}

#[derive(Debug, Clone)]
pub enum LazySemanticContext {
    /// Not yet analyzed
    Pending,
    /// Currently being analyzed
    InProgress,
    /// Analysis complete
    Complete(SemanticContext),
    /// Analysis failed
    Failed(String),
}

impl LazySemanticAnalysis {
    /// Get semantic context, analyzing if needed
    pub fn get_semantic_context(&mut self, token_id: TokenId) -> Result<&SemanticContext, LexerError> {
        match &self.tokens[token_id].semantic_context {
            LazySemanticContext::Complete(context) => Ok(context),
            LazySemanticContext::Pending => {
                // Analyze now
                self.analyze_token(token_id)?;
                self.get_semantic_context(token_id)
            }
            LazySemanticContext::InProgress => {
                // Wait for analysis to complete
                self.wait_for_analysis(token_id)?;
                self.get_semantic_context(token_id)
            }
            LazySemanticContext::Failed(error) => {
                Err(LexerError::SemanticAnalysisFailed(error.clone()))
            }
        }
    }
}
```

#### 9.2 Parallel Processing

```rust
/// Parallel semantic analysis for performance
pub struct ParallelSemanticAnalyzer {
    /// Thread pool for analysis
    thread_pool: ThreadPool,
    /// Analysis tasks
    tasks: Vec<AnalysisTask>,
    /// Results channel
    results: mpsc::Receiver<AnalysisResult>,
}

#[derive(Debug)]
pub struct AnalysisTask {
    pub token_id: TokenId,
    pub token: Token,
    pub context: AnalysisContext,
}

#[derive(Debug)]
pub struct AnalysisResult {
    pub token_id: TokenId,
    pub result: Result<SemanticContext, LexerError>,
}

impl ParallelSemanticAnalyzer {
    /// Analyze tokens in parallel
    pub fn analyze_parallel(&mut self, tokens: Vec<Token>) -> Result<Vec<Token>, LexerError> {
        let mut enriched_tokens = Vec::with_capacity(tokens.len());
        
        // Submit analysis tasks
        for (id, token) in tokens.into_iter().enumerate() {
            let task = AnalysisTask {
                token_id: id,
                token: token.clone(),
                context: AnalysisContext::new(),
            };
            
            self.thread_pool.execute(move || {
                let result = self.analyze_token_semantic_context(&task.token);
                // Send result back
            });
            
            enriched_tokens.push(token);
        }
        
        // Collect results
        for _ in 0..enriched_tokens.len() {
            let result = self.results.recv()
                .map_err(|e| LexerError::AnalysisError(e.to_string()))?;
                
            match result.result {
                Ok(context) => {
                    enriched_tokens[result.token_id].semantic_context = Some(context);
                }
                Err(error) => {
                    return Err(error);
                }
            }
        }
        
        Ok(enriched_tokens)
    }
}
```

## Open Issues

### 10. Future Enhancements

#### 10.1 Advanced Semantic Analysis

1. **Machine Learning Integration**: Use ML models to improve semantic context inference
2. **Cross-Module Analysis**: Analyze semantic relationships across module boundaries
3. **Business Rule Extraction**: Automatically extract business rules from code patterns
4. **Compliance Checking**: Automated compliance checking during lexing

#### 10.2 Performance Optimizations

1. **Streaming Lexing**: Support for streaming large files
2. **Incremental Semantic Analysis**: More granular incremental updates
3. **Memory Optimization**: Reduce memory footprint for large codebases
4. **Parallel Documentation Validation**: Parallel validation of documentation requirements

#### 10.3 Developer Experience

1. **Real-time Feedback**: Instant semantic feedback as developers type
2. **Intelligent Suggestions**: Context-aware suggestions for missing annotations
3. **Refactoring Support**: Semantic-aware refactoring suggestions
4. **AI-Powered Documentation**: Automatic generation of documentation from semantic context

## References

1. **[Logos Crate](https://crates.io/crates/logos)** - Fast lexer generator for Rust
2. **[Tree-sitter](https://tree-sitter.github.io/)** - Incremental parsing library
3. **[Language Server Protocol](https://microsoft.github.io/language-server-protocol/)** - LSP specification
4. **[Semantic Versioning](https://semver.org/)** - Version management
5. **[Unicode Standard](https://unicode.org/standard/standard.html)** - Character encoding
6. **[PSG-001: Fundamental Syntax & Formatting](../PSG/PSG-001.md)** - Syntax style guidelines
7. **[PSG-002: Naming Conventions & Identifiers](../PSG/PSG-002.md)** - Naming and responsibility standards
8. **[PSG-003: PrismDoc Standards](../PSG/PSG-003.md)** - Documentation requirements
9. **[PLD-001: Semantic Type System](../PLD/PLD-001.md)** - Semantic type integration
10. **[PLD-002: Smart Module System](../PLD/PLD-002.md)** - Conceptual cohesion metrics
11. **[PLD-003: Effect System & Capabilities](../PLD/PLD-003.md)** - Effect system integration
12. **[PLD-004: Compiler Architecture](../PLD/PLD-004.md)** - Query-based compilation
13. **[PLT-001: AST Design & Parser Architecture](./PLT-001.md)** - Parser integration

## Appendices

### Appendix A: Token Type Reference

Complete reference of all token types, their semantic meanings, and AI context:

```rust
// See implementation above for complete token type definitions
```

### Appendix B: Semantic Context Examples

Examples of semantic context for different token types:

```rust
// See implementation above for complete semantic context examples
```

### Appendix C: Performance Benchmarks

Performance characteristics of the lexer:

- **Basic Tokenization**: ~50MB/s
- **Semantic Analysis**: ~20MB/s
- **Documentation Validation**: ~15MB/s
- **Cohesion Metrics**: ~10MB/s
- **Memory Usage**: ~100MB per 1M tokens

### Appendix D: Error Recovery Strategies

Complete list of error recovery strategies by error type:

```rust
// See implementation above for complete error recovery strategies
```

---

## Document History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 0.1.0 | 2025-01-17 | Team | Initial lexical analysis and tokenization specification with full system integration |

## Review Sign-offs

| Reviewer | Role | Status | Date |
|----------|------|--------|------|
| - | Language Design | Pending | - |
| - | Compiler Architecture | Pending | - |
| - | AI Integration | Pending | - |
| - | Performance | Pending | - |
| - | Documentation | Pending | - | 